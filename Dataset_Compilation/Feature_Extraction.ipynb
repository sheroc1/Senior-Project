{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742ba44-8cf2-4c1f-96a2-96955a434720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from Bio.PDB import PDBParser, PDBList\n",
    "from Bio.PDB.SASA import ShrakeRupley\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# Define file paths\n",
    "input_file = \"/Users/carmenshero/Desktop/Datasets/Training_Set.csv\"\n",
    "\n",
    "# Load dataset correctly (remove duplicate `pd.read_csv`)\n",
    "df_training = pd.read_csv(input_file, dtype={\"ec_numbers\": str, \"sequence\": str}, low_memory=False)\n",
    "\n",
    "# Replace \"nan\" and NaN with an empty string (ensures missing values are recognized)\n",
    "df_training[\"ec_numbers\"] = df_training[\"ec_numbers\"].replace([\"nan\", np.nan], \"\")\n",
    "df_training[\"sequence\"] = df_training[\"sequence\"].replace([\"nan\", np.nan], \"\")\n",
    "\n",
    "# Fix \"Processed\" logic to check only C-P but always reprocess empty sequences\n",
    "df_training[\"Processed\"] = df_training.iloc[:, 2:23].notnull().all(axis=1) & df_training[\"sequence\"].str.strip().astype(bool)\n",
    "\n",
    "# Identify rows that still need processing\n",
    "remaining_rows = df_training[~df_training[\"Processed\"]].copy()\n",
    "df_training.drop(columns=[\"Processed\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0258c2-1a0b-4997-9334-c218a7c8583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch UniProt info\n",
    "def get_protein_info(uniprot_id):\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    return None\n",
    "\n",
    "# Function to get protein sequence\n",
    "def get_protein_sequence(uniprot_id):\n",
    "    data = get_protein_info(uniprot_id)\n",
    "    if data:\n",
    "        return data.get('sequence', {}).get('value', 'N/A')\n",
    "    return 'N/A'\n",
    "\n",
    "# Function to get EC numbers\n",
    "def extract_ec_numbers(protein_info):\n",
    "    ec_numbers = []\n",
    "    if protein_info:\n",
    "        for comment in protein_info.get('comments', []):\n",
    "            if comment.get('commentType') == 'CATALYTIC ACTIVITY':\n",
    "                reaction = comment.get('reaction', {})\n",
    "                ec_number = reaction.get('ecNumber')\n",
    "                if ec_number:\n",
    "                    ec_numbers.append(ec_number)\n",
    "    return \";\".join(ec_numbers) if ec_numbers else \"MISSING\"\n",
    "\n",
    "# Function to get secondary structure info\n",
    "def get_pdbml(pdb_id):\n",
    "    url = f\"https://files.rcsb.org/view/{pdb_id}.xml\"\n",
    "    response = requests.get(url)\n",
    "    return response.content if response.status_code == 200 else None\n",
    "\n",
    "def parse_secondary_structure(pdbml_content):\n",
    "    if not pdbml_content:\n",
    "        return []\n",
    "    \n",
    "    root = ET.fromstring(pdbml_content)\n",
    "    ns = {'pdbx': 'http://pdbml.pdb.org/schema/pdbx-v50.xsd'}\n",
    "    sec_struct = []\n",
    "\n",
    "    for ss in root.findall('.//pdbx:struct_conf', ns):\n",
    "        start_res = ss.find('pdbx:beg_auth_seq_id', ns).text\n",
    "        end_res = ss.find('pdbx:end_auth_seq_id', ns).text\n",
    "        conf_type = ss.find('pdbx:conf_type_id', ns).text\n",
    "        sec_struct.append((int(start_res), int(end_res), conf_type))\n",
    "    \n",
    "    for ss in root.findall('.//pdbx:struct_sheet_range', ns):\n",
    "        start_res = ss.find('pdbx:beg_auth_seq_id', ns).text\n",
    "        end_res = ss.find('pdbx:end_auth_seq_id', ns).text\n",
    "        sec_struct.append((int(start_res), int(end_res), 'STRAND'))\n",
    "    \n",
    "    return sec_struct\n",
    "\n",
    "# Calculate surface area\n",
    "def Get_SA(pdb_id):\n",
    "    url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
    "    file_path = f\"/Users/carmenshero/Desktop/{pdb_id}.pdb\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return np.nan\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_id, file_path)\n",
    "    sr = ShrakeRupley()\n",
    "    sr.compute(structure, level=\"S\")\n",
    "    surface_area = structure.sasa\n",
    "\n",
    "    os.remove(file_path)  # Cleanup\n",
    "    return int(surface_area)\n",
    "\n",
    "# Define Amino Acid Groups\n",
    "aa_groups = {\n",
    "    'Hydrophobic': ['A', 'V', 'I', 'L', 'M', 'F', 'W', 'Y'],\n",
    "    'Polar': ['S', 'T', 'N', 'Q', 'C'],\n",
    "    'Basic': ['K', 'R', 'H'],\n",
    "    'Acidic': ['D', 'E'],\n",
    "    'Special': ['G', 'P']\n",
    "}\n",
    "\n",
    "# Function to Calculate Amino Acid Composition\n",
    "def amino_acid_composition(sequence):\n",
    "    if sequence == \"N/A\" or sequence == \"\":\n",
    "        return {aa: 0.0 for aa in aa_groups['Hydrophobic'] + aa_groups['Polar'] +\n",
    "                               aa_groups['Basic'] + aa_groups['Acidic'] + aa_groups['Special']}\n",
    "\n",
    "    amino_acid_counts = {aa: 0 for aa in aa_groups['Hydrophobic'] + aa_groups['Polar'] +\n",
    "                                    aa_groups['Basic'] + aa_groups['Acidic'] + aa_groups['Special']}\n",
    "    \n",
    "    for amino_acid in sequence:\n",
    "        if amino_acid in amino_acid_counts:\n",
    "            amino_acid_counts[amino_acid] += 1\n",
    "    \n",
    "    sequence_length = len(sequence)\n",
    "    if sequence_length == 0:\n",
    "        return {aa: 0.0 for aa in amino_acid_counts}\n",
    "\n",
    "    return {aa: (count / sequence_length) * 100 for aa, count in amino_acid_counts.items()}\n",
    "\n",
    "# Function to Group Amino Acid Frequencies into Categories\n",
    "def group_aa_frequencies(aa_percent):\n",
    "    grouped_data = {group: 0 for group in aa_groups.keys()}\n",
    "    for aa, freq in aa_percent.items():\n",
    "        for group, aas in aa_groups.items():\n",
    "            if aa in aas:\n",
    "                grouped_data[group] += freq\n",
    "                break\n",
    "    return grouped_data\n",
    "\n",
    "# Calculate molecular weight and hydrophobicity\n",
    "def calculate_physicochemical_properties(sequence):\n",
    "    if sequence == \"N/A\":\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    analysed_seq = ProteinAnalysis(sequence)\n",
    "    return analysed_seq.molecular_weight(), analysed_seq.gravy()\n",
    "\n",
    "# Calculate number of chains (also needed for SA_div_total_seq)\n",
    "def get_num_of_chains(pdb_id):\n",
    "    \"\"\"Retrieve the number of chains from a PDB file.\"\"\"\n",
    "    pdbl = PDBList()\n",
    "    pdb_file = pdbl.retrieve_pdb_file(pdb_id, file_format='pdb')\n",
    "    \n",
    "    # Parse the PDB file\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_id, pdb_file)\n",
    "    \n",
    "    # Count unique chain IDs\n",
    "    num_chains = len(set(chain.id for model in structure for chain in model))\n",
    "    \n",
    "    # Clean up: Delete PDB file after use\n",
    "    os.remove(pdb_file)\n",
    "\n",
    "    return num_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31721cf6-e5c1-43f7-b9fc-df5a3e0b6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each row\n",
    "for index, row in remaining_rows.iterrows():\n",
    "    pdb_id, uniprot_id = row[\"PDB_ID\"], row[\"UniProt_ID\"]\n",
    "\n",
    "    # ❌ Skip rows with missing UniProt ID\n",
    "    if not uniprot_id or pd.isna(uniprot_id):\n",
    "        print(f\"❌ Skipping {pdb_id} due to missing UniProt ID.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Fetch data\n",
    "        protein_info = get_protein_info(uniprot_id)\n",
    "        sequence = get_protein_sequence(uniprot_id)\n",
    "        secondary_structure = parse_secondary_structure(get_pdbml(pdb_id))\n",
    "        surface_area = Get_SA(pdb_id)\n",
    "        num_of_chains = get_num_of_chains(pdb_id)\n",
    "\n",
    "        # Calculate Amino Acid Composition\n",
    "        aa_percent = amino_acid_composition(sequence)\n",
    "        grouped_frequencies = group_aa_frequencies(aa_percent)\n",
    "\n",
    "        # Calculate structural properties\n",
    "        total_helix_length = sum(end - start + 1 for start, end, conf in secondary_structure if \"HELX\" in conf)\n",
    "        total_strand_length = sum(end - start + 1 for start, end, conf in secondary_structure if conf == \"STRAND\")\n",
    "        sequence_length = len(sequence) if sequence != \"N/A\" else np.nan\n",
    "        percent_helix = round((total_helix_length / sequence_length) * 100, 2) if sequence_length else np.nan\n",
    "        percent_strand = round((total_strand_length / sequence_length) * 100, 2) if sequence_length else np.nan\n",
    "        longest_helix_length = max((end - start + 1 for start, end, conf in secondary_structure if \"HELX\" in conf), default=0)\n",
    "        longest_strand_length = max((end - start + 1 for start, end, conf in secondary_structure if conf == \"STRAND\"), default=0)\n",
    "        molecular_weight, hydrophobicity = calculate_physicochemical_properties(sequence)\n",
    "\n",
    "        # Calculate SA_div_total_seq\n",
    "        if num_of_chains > 0 and sequence_length > 0:  # Avoid division by zero\n",
    "            SA_div_total_seq = surface_area / (sequence_length / num_of_chains)\n",
    "        else:\n",
    "            SA_div_total_seq = np.nan  # Assign NaN if division is invalid\n",
    "\n",
    "        # Store updated values in the dataframe\n",
    "        df_training.at[index, \"Surface_area\"] = surface_area\n",
    "        df_training.at[index, \"sequence_length\"] = sequence_length\n",
    "        df_training.at[index, \"num_of_helicies\"] = sum(1 for _, _, conf in secondary_structure if \"HELX\" in conf)\n",
    "        df_training.at[index, \"num_of_strand\"] = sum(1 for _, _, conf in secondary_structure if conf == \"STRAND\")\n",
    "        df_training.at[index, \"total_helix_length\"] = total_helix_length\n",
    "        df_training.at[index, \"total_Strand_length\"] = total_strand_length\n",
    "        df_training.at[index, \"percent_helix\"] = percent_helix\n",
    "        df_training.at[index, \"percent_strand\"] = percent_strand\n",
    "        df_training.at[index, \"longest_helix_length\"] = longest_helix_length\n",
    "        df_training.at[index, \"longest_strand_length\"] = longest_strand_length\n",
    "        df_training.at[index, \"Molecular_weight\"] = molecular_weight\n",
    "        df_training.at[index, \"Hydrophobicity\"] = hydrophobicity\n",
    "        df_training.at[index, \"ec_numbers\"] = extract_ec_numbers(protein_info)  # Always returns a string\n",
    "        df_training.at[index, \"sequence\"] = sequence  # Store protein sequence\n",
    "        \n",
    "        # Assign Amino Acid Percentages (Columns Q-U)\n",
    "        df_training.at[index, \"Hydrophobic_AA_percent\"] = grouped_frequencies[\"Hydrophobic\"]\n",
    "        df_training.at[index, \"Polar_AA_percent\"] = grouped_frequencies[\"Polar\"]\n",
    "        df_training.at[index, \"Basic_AA_percent\"] = grouped_frequencies[\"Basic\"]\n",
    "        df_training.at[index, \"Acidic_AA_percent\"] = grouped_frequencies[\"Acidic\"]\n",
    "        df_training.at[index, \"Special_AA_percent\"] = grouped_frequencies[\"Special\"]\n",
    "        df_training.at[index, \"Num_of_Chains\"] = num_of_chains\n",
    "        df_training.at[index, \"SA_div_total_seq\"] = SA_div_total_seq\n",
    "        \n",
    "        # Save progress after processing each row\n",
    "        df_training.to_csv(input_file, index=False)  \n",
    "        print(f\"✔ Updated {pdb_id} ({uniprot_id}) and saved progress.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {pdb_id} ({uniprot_id}): {e}\")\n",
    "        continue  # Skip this row and move to the next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da2f35-978a-4435-88cc-4fcd8011560b",
   "metadata": {},
   "source": [
    "BELOW IS POSSIBLE MOTIF FINDER / ONE HOT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20531b80-7506-423d-aad9-c31c9d62c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract motifs for each protein\n",
    "def get_protein_motifs(uniprot_id):\n",
    "    \"\"\"Retrieve motifs (PROSITE signatures) for a given UniProt ID.\"\"\"\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"⚠️ Failed to retrieve motifs for {uniprot_id}\")\n",
    "        return []\n",
    "    \n",
    "    data = response.json()\n",
    "    motifs = []\n",
    "\n",
    "    # Extract motifs from \"crossReferences\" (PROSITE domain)\n",
    "    for xref in data.get('uniProtKBCrossReferences', []):\n",
    "        if xref.get(\"database\") == \"PROSITE\":\n",
    "            motifs.append(xref.get(\"id\"))  # Signature AC (e.g., \"PS00010\")\n",
    "\n",
    "    return motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145f943-e2e2-4c5e-82cc-c95438fc5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # ✅ Add time delay to avoid API rate limits\n",
    "\n",
    "# Load dataset\n",
    "df_training = pd.read_csv(input_file, dtype={\"ec_numbers\": str, \"sequence\": str}, low_memory=False)\n",
    "\n",
    "# ✅ Collect all unique motifs across dataset with rate limiting\n",
    "all_unique_motifs = set()\n",
    "\n",
    "for index, row in df_training.iterrows():\n",
    "    uniprot_id = row[\"UniProt_ID\"]\n",
    "    motifs = get_protein_motifs(uniprot_id)\n",
    "    all_unique_motifs.update(motifs)\n",
    "    \n",
    "    # ✅ Slow down requests to avoid being blocked (every 100 requests)\n",
    "    if index % 100 == 0:\n",
    "        time.sleep(1)  # 1-second pause after every 100 requests\n",
    "\n",
    "# ✅ Convert to sorted list for column consistency\n",
    "all_unique_motifs = sorted(list(all_unique_motifs))\n",
    "\n",
    "# ✅ Add motif columns to DataFrame\n",
    "for motif in all_unique_motifs:\n",
    "    if motif not in df_training.columns:\n",
    "        df_training[motif] = 0  # Initialize with zeros\n",
    "\n",
    "# ✅ Save updated dataset\n",
    "df_training.to_csv(input_file, index=False)\n",
    "print(f\"✔ Found {len(all_unique_motifs)} unique motifs. Added columns and saved dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970baa66-8dad-4d31-8b4e-639cbbeceeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000  # ✅ Save in batches of 1,000 for efficiency\n",
    "batch_updates = 0  # Counter for batch saving\n",
    "\n",
    "# Load dataset\n",
    "df_training = pd.read_csv(input_file, dtype={\"ec_numbers\": str, \"sequence\": str}, low_memory=False)\n",
    "\n",
    "# ✅ Create \"Processed_Motifs\" column if missing\n",
    "if \"Processed_Motifs\" not in df_training.columns:\n",
    "    df_training[\"Processed_Motifs\"] = False\n",
    "\n",
    "# ✅ Process each row individually (ensures crash recovery)\n",
    "for index, row in df_training.iterrows():\n",
    "    uniprot_id = row[\"UniProt_ID\"]\n",
    "    \n",
    "    # ✅ Skip already processed rows\n",
    "    if row[\"Processed_Motifs\"]:\n",
    "        continue\n",
    "\n",
    "    # Skip missing UniProt IDs\n",
    "    if not uniprot_id or pd.isna(uniprot_id):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        motifs = get_protein_motifs(uniprot_id)\n",
    "\n",
    "        # ✅ Update one-hot encoded columns\n",
    "        for motif in motifs:\n",
    "            if motif in df_training.columns:\n",
    "                df_training.at[index, motif] = 1  # Mark presence\n",
    "\n",
    "        # ✅ Mark row as processed\n",
    "        df_training.at[index, \"Processed_Motifs\"] = True  \n",
    "\n",
    "        batch_updates += 1\n",
    "\n",
    "        # ✅ Save every `batch_size` rows (to avoid slow disk I/O)\n",
    "        if batch_updates % batch_size == 0:\n",
    "            df_training.to_csv(input_file, index=False)\n",
    "            print(f\"✔ Saved batch of {batch_size} rows.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {uniprot_id}: {e}\")\n",
    "        continue  # Skip to the next row if an error occurs\n",
    "\n",
    "# ✅ Final save (for remaining rows)\n",
    "df_training.to_csv(input_file, index=False)\n",
    "print(\"✔ One-hot encoding completed. Dataset updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
