{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b8e27f-843e-40a8-851e-4fd5783b98e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded dataset with 85 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4n/4sxtv64s2lvd9sr2rmx2d0f00000gn/T/ipykernel_9547/129623869.py:11: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors='ignore')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define file path\n",
    "input_file = \"/Users/carmenshero/Desktop/Datasets/initialSet.csv\"\n",
    "\n",
    "# Load full dataset (including motifs)\n",
    "df = pd.read_csv(input_file, dtype=str, low_memory=False)\n",
    "\n",
    "# Convert numeric columns to proper dtypes after initial load\n",
    "df = df.apply(pd.to_numeric, errors='ignore')\n",
    "print(f\" Loaded dataset with {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c88a2aa-8632-452e-96c4-06517da50cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Filtered down to 85 complete rows.\n"
     ]
    }
   ],
   "source": [
    "# Filter out any rows that have missing data\n",
    "df_filtered = df[df.iloc[:, :23].notnull().all(axis=1)].copy()\n",
    "\n",
    "print(f\" Filtered down to {len(df_filtered)} complete rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfbffe6d-f290-45c3-bd77-826a436547fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Standardized columns Câ€“N and Qâ€“W (excluding ec_numbers & sequence).\n"
     ]
    }
   ],
   "source": [
    "# Standardizing the numeric features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Columns Câ€“N: index 2â€“13\n",
    "# Columns Qâ€“W: index 16â€“22\n",
    "standardize_cols = list(df_filtered.columns[2:14]) + list(df_filtered.columns[16:23])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_filtered[standardize_cols] = scaler.fit_transform(df_filtered[standardize_cols].astype(float))\n",
    "\n",
    "print(f\" Standardized columns Câ€“N and Qâ€“W (excluding ec_numbers & sequence).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da385890-0a64-4ddf-80da-73d20d1e9c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA complete â€” 10 PC columns added.\n"
     ]
    }
   ],
   "source": [
    "#PCA'ing the motif columns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Identify motif columns (between W and Processed_Motifs)\n",
    "motif_cols = df_filtered.columns[23:-1]  # Assumes column 23 starts motifs, last col is Processed_Motifs\n",
    "motif_data = df_filtered[motif_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Perform PCA (Feel free to change this to as many or as little columns as you want)\n",
    "pca = PCA(n_components=10)\n",
    "motif_pca = pca.fit_transform(motif_data)\n",
    "\n",
    "# Create a DataFrame for PCA results and merge it\n",
    "pca_cols = [f\"PC{i}\" for i in range(1, 11)]\n",
    "pca_df = pd.DataFrame(motif_pca, columns=pca_cols, index=df_filtered.index)\n",
    "\n",
    "# Drop existing PCA columns if they already exist (to prevent partial overwrite)\n",
    "df_filtered.drop(columns=[col for col in pca_cols if col in df_filtered.columns], inplace=True)\n",
    "\n",
    "# Append new PCA columns\n",
    "df_filtered = pd.concat([df_filtered, pca_df], axis=1)\n",
    "\n",
    "print(\"PCA complete â€” 10 PC columns added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d30698-b263-442f-86bd-f24990d2d83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved 43 rows to Training.csv\n",
      "ðŸ’¾ Saved 42 rows to Prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# Dropping the motif columns and Processed_Motifs + sequence columns before saving\n",
    "# Them to Training and Prediction files (ec_numbers == MISSING goes to Prediction.csv)\n",
    "# Additionally moves ec_numbers to column C for easier model training / target feature choosing\n",
    "\n",
    "# Define important columns to retain\n",
    "pca_cols = [f\"PC{i}\" for i in range(1, 11)]\n",
    "core_cols = [\"PDB_ID\", \"UniProt_ID\", \"ec_numbers\"]  # We'll reorder to place ec_numbers third\n",
    "feature_cols = [col for col in df_filtered.columns[2:23] if col != \"sequence\"]\n",
    "columns_to_keep = core_cols + feature_cols + pca_cols\n",
    "\n",
    "# Drop unwanted columns: motifs, sequence, Processed_Motifs, extras\n",
    "columns_to_drop = [col for col in df_filtered.columns if col not in columns_to_keep]\n",
    "\n",
    "# Create training and prediction DataFrames\n",
    "df_training = df_filtered[df_filtered[\"ec_numbers\"] != \"MISSING\"].copy()\n",
    "df_prediction = df_filtered[df_filtered[\"ec_numbers\"] == \"MISSING\"].copy()\n",
    "\n",
    "# Drop unneeded columns\n",
    "df_training.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n",
    "df_prediction.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Reorder columns: ec_numbers as column C (index 2)\n",
    "ordered_columns = [\"PDB_ID\", \"UniProt_ID\", \"ec_numbers\"] + [col for col in df_training.columns if col not in [\"PDB_ID\", \"UniProt_ID\", \"ec_numbers\"]]\n",
    "df_training = df_training[ordered_columns]\n",
    "df_prediction = df_prediction[ordered_columns]\n",
    "\n",
    "# Save to CSV\n",
    "training_file = \"/Users/carmenshero/Desktop/Datasets/Training.csv\"\n",
    "prediction_file = \"/Users/carmenshero/Desktop/Datasets/Prediction.csv\"\n",
    "\n",
    "df_training.to_csv(training_file, index=False)\n",
    "df_prediction.to_csv(prediction_file, index=False)\n",
    "\n",
    "print(f\"ðŸ’¾ Saved {len(df_training)} rows to Training.csv\")\n",
    "print(f\"ðŸ’¾ Saved {len(df_prediction)} rows to Prediction.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
